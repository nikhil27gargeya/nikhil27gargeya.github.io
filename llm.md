# LLM


LLMs are neural networks (informs the architecture to use), they utilize training loss algorithms, evaluation (progress), systems (how to make the models run on hardware), transformers (text converted to numerical representaitons called tokens)

Language models at a high level are probability distribution over word sequences P(the, dog, and, the, cat) = probability of the sentences appearing online and they shows the syntactic and semantic knowledge

Generative models: once there is model of a distribution, you can sample this model and generate data

Autoregressive language models: predict next token given past content, not the only way to modeling distribution


